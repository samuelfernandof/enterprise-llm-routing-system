# Enterprise LLM Routing System

<div align="center">
  <img src="https://github.com/samuelfernandof/enterprise-llm-routing-system/blob/main/llm-router/src/Captura%20de%20tela%202024-10-25%20112950.png" width="300" alt="Logo do projeto">
</div>

### Estrutura do projeto
```bash
llm-router/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â””â”€â”€ middleware.py
â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ route_controller.py
â”‚   â”‚   â”œâ”€â”€ auth_controller.py
â”‚   â”‚   â””â”€â”€ prompt_controller.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ request_model.py
â”‚   â”‚   â”œâ”€â”€ response_model.py
â”‚   â”‚   â””â”€â”€ llm_model.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routing/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ route_manager.py
â”‚   â”‚   â”‚   â”œâ”€â”€ load_balancer.py
â”‚   â”‚   â”‚   â””â”€â”€ request_router.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ prompt_manager.py
â”‚   â”‚   â”‚   â”œâ”€â”€ context_manager.py
â”‚   â”‚   â”‚   â””â”€â”€ prompt_engine.py
â”‚   â”‚   â””â”€â”€ providers/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ base.py
â”‚   â”‚       â”œâ”€â”€ anthropic.py
â”‚   â”‚       â”œâ”€â”€ openai.py
â”‚   â”‚       â””â”€â”€ google.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ cache.py
â”‚   â”‚   â””â”€â”€ queue.py
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ settings.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_controllers/
â”‚   â”œâ”€â”€ test_models/
â”‚   â””â”€â”€ test_core/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ api.yaml
â”œâ”€â”€ requirements.txt
â””â”€â”€ main.py




```

### Arquitetura do Sistema

## 1. VIEW LAYER (Camada de ApresentaÃ§Ã£o)
- **Web Interface/API Gateway**: Ponto de entrada Ãºnico para todas as requisiÃ§Ãµes
- **API Documentation**: DocumentaÃ§Ã£o OpenAPI/Swagger para os endpoints

## 2. CONTROLLER LAYER (Camada de Controle)
- **Route Controller**: Gerencia o roteamento inicial das requisiÃ§Ãµes
- **Auth Controller**: Controle de autenticaÃ§Ã£o e autorizaÃ§Ã£o
- **Prompt Controller**: Gerenciamento dos prompts e inputs

## 3. MODEL LAYER (Camada de Modelo/NegÃ³cios)

### A. ROUTING ENGINE (ðŸ”€ Core do Sistema)
- **Route Manager**: Gerencia as regras de roteamento
- **Load Balancer**: Distribui carga entre os LLMs
- **Request Router**: Determina qual LLM usar baseado em regras especÃ­ficas

### B. CORE SERVICES
- **Prompt Manager**: Gerencia e otimiza os prompts
- **Context Manager**: MantÃ©m e gerencia o contexto das conversas
- **Prompt Engine**: Processa e prepara os prompts para os LLMs

### C. LLM PROVIDERS
IntegraÃ§Ãµes com diferentes LLMs:
* Claude (Anthropic)
* GPT-4 (OpenAI)
* PaLM (Google)
* Gemini (Google)

### D. DATA LAYER
- **Database**: Armazenamento persistente
- **Redis Cache**: Cache para respostas e contextos
- **Message Queue**: Filas para processamento assÃ­ncrono

## Fluxo de Dados:
1. RequisiÃ§Ã£o chega via API Gateway
2. Passa pela autenticaÃ§Ã£o
3. Route Controller direciona para o Route Manager
4. Load Balancer verifica disponibilidade dos LLMs
5. Request Router seleciona o LLM mais apropriado baseado em:
   - Regras de negÃ³cio
   - Disponibilidade
   - Capacidades especÃ­ficas
   - Custos
   - Performance
6. Prompt Engine prepara o prompt final
7. Resposta Ã© processada e cacheada se necessÃ¡rio
8. Resultado retorna ao usuÃ¡rio

## BenefÃ­cios desta Arquitetura:
1. Escalabilidade horizontal e vertical
2. Alta disponibilidade
3. Isolamento de responsabilidades
4. Facilidade de manutenÃ§Ã£o
5. Possibilidade de failover entre LLMs
6. Gerenciamento eficiente de recursos
7. Cache inteligente de respostas
8. Processamento assÃ­ncrono quando necessÃ¡rio

Esta arquitetura Ã© particularmente eficiente para sistemas de LLM Routing porque:
- Permite balanceamento de carga inteligente
- Facilita a adiÃ§Ã£o de novos LLMs
- Gerencia eficientemente os custos
- MantÃ©m o contexto das conversas
- Permite otimizaÃ§Ã£o de prompts
- Oferece alta disponibilidade
